{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/nas2/biod/zhencaiwei/RegChat-main/Code/')\n",
    "from DGI_HGCN.models import DGI, LogReg\n",
    "from DGI_HGCN.utils import process\n",
    "from Plot import Visualization\n",
    "import pickle as pkl\n",
    "import ast\n",
    "import pandas as pd\n",
    "import random\n",
    "from Attscore_mat import RegChat_HDGI_train, data_preprocess, compute_attscore_mat\n",
    "import json \n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import lil_matrix\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "import math\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "from scipy import stats\n",
    "\n",
    "torch.backends.cudnn.enable =True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] ='1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/'\n",
    "path_nt = '/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "signaling_pathway = pd.read_csv(path + 'signaling_pathway.csv')\n",
    "gene_cell_mat = pd.read_csv(path_nt + 'simulated_adata_ST_00.csv',sep=',',index_col=0)\n",
    "coord = pd.read_csv(path_nt + 'coordinates_df.csv',sep='\\t')\n",
    "cell_type = pd.read_csv(path_nt + 'spot_CT1_CT3_label_str.txt',sep='\\t')\n",
    "cell_type_num = pd.read_csv(path_nt + 'spot_CT1_CT3_label_num.txt',sep='\\t')\n",
    "adjs_names_list = np.loadtxt(path_nt+'simulated_adj_files_all.txt', dtype='str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate adj matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:00<00:00, 3751.40it/s]\n",
      "100%|██████████| 31/31 [00:00<00:00, 1864.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 490 cells\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L0_R3_F12_G9_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L0_R8_F12_G15_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L1_R1_F1_G1_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L1_R6_F17_G23_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L2_R0_F4_G4_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L3_R0_F4_G4_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L4_R3_F5_G6_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L4_R5_F5_G7_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L5_R7_F11_G17_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L5_R7_F11_G32_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L6_R9_F9_G2_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L7_R4_F3_G4_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L7_R3_F10_G5_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L7_R7_F11_G17_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L7_R7_F11_G32_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L7_R4_F10_G35_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L8_R2_F11_G17_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L8_R2_F11_G32_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L9_R0_F4_G4_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L10_R2_F11_G17_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L10_R2_F11_G32_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L10_R5_F3_G4_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L11_R1_F11_G17_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L11_R1_F11_G32_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L12_R1_F15_G1_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L13_R0_F4_G4_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L13_R2_F11_G17_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L13_R2_F11_G32_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L13_R6_F4_G4_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L14_R7_F8_G10_adj.pickle has been saved\n",
      "/home/nas2/biod/zhencaiwei/RegChat-main/Simudata/simulated_data_000/clrfgc_adj/L15_R0_F4_G4_adj.pickle has been saved\n"
     ]
    }
   ],
   "source": [
    "compute_attscore_mat.generate_adj_matrix(gene_cell_mat, signaling_pathway, path_nt, coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get attscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scell_types = ['CT3']\n",
    "rcell_types = ['CT3']\n",
    "outs_symbol = path_nt + 'outs/'\n",
    "os.makedirs(outs_symbol, exist_ok=True) \n",
    "adj_dir = path_nt + 'clrfgc_adj/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----hdgi_nb_class----- 3\n"
     ]
    }
   ],
   "source": [
    "'''load adjs matrix'''\n",
    "adjs = []\n",
    "for adj_name in adjs_names_list:\n",
    "    with open(adj_dir + adj_name, 'rb') as f:\n",
    "        adj = pkl.load(f)\n",
    "    f.close\n",
    "    adjs.append(adj) \n",
    "'''load labels csv, labels----->one hot encoding'''\n",
    "numeric_labels = cell_type_num['label_num']\n",
    "hdgi_labels = torch.tensor(numeric_labels)\n",
    "numeric_labels_set = set(numeric_labels)\n",
    "hdgi_nb_class = len(numeric_labels_set)\n",
    "print('----hdgi_nb_class-----', hdgi_nb_class)    \n",
    "\n",
    "'''create temp dirs'''\n",
    "current_datetime = datetime.datetime.now()\n",
    "current_time_str = current_datetime.strftime(\"%Y-%m-%d-%H-%M-%S/\")\n",
    "folder_name = current_time_str\n",
    "path_temp = os.path.join(outs_symbol+'clrfgc_temp/',  folder_name)\n",
    "os.makedirs(path_temp)\n",
    "    \n",
    "metapath_attention_lists = {metapath: [] for metapath in adjs_names_list}\n",
    "metapath_attention_zscore_lists = {metapath: [] for metapath in adjs_names_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sc_type in scell_types:\n",
    "    for rc_type in rcell_types:\n",
    "        masked_gene_cell_mat, df_cell, fixed_idx = compute_attscore_mat.generate_cell_node(sc_type, rc_type, \n",
    "                                                                                           signaling_pathway, \n",
    "                                                                                           gene_cell_mat, cell_type)\n",
    "        float_path_weight = [1]*len(signaling_pathway)\n",
    "        \n",
    "        del masked_gene_cell_mat\n",
    "        print('sc_type', sc_type)\n",
    "        print('rc_type', rc_type)\n",
    "        print('float_path_weight', float_path_weight)\n",
    "\n",
    "\n",
    "        '''Convert the feature types in the dataframe, converting strings to lists'''\n",
    "        cell_fea = []\n",
    "        for line in df_cell['feature']:\n",
    "            line = [float(i) for i in line]\n",
    "            cell_fea.append(line)\n",
    "\n",
    "\n",
    "        '''traning params'''\n",
    "        hdgi_para_dict = {\n",
    "            'original_idx_len':gene_cell_mat.shape[1],\n",
    "            'train_idx_len':300,#int(gene_cell_mat.shape[1]*0.2)\n",
    "            'test_idx_len':100,\n",
    "            'batch_size': 1,\n",
    "            'nb_epochs': 10000,\n",
    "            'patience': 50,\n",
    "            'lr': 0.0008,\n",
    "            'l2_coef': 0.0,\n",
    "            'drop_prob': 0.0,\n",
    "            'hid_units': 64, # output of the GCN dimension\n",
    "            'shid': 16, # input dimension of semantic-level attention\n",
    "            'sparse': True,\n",
    "            'preprocess_feat': True,\n",
    "            'nonlinearity': 'prelu', # special name to separate parameters\n",
    "            'LRpair': 'pathway',\n",
    "            'noise_value': 0,\n",
    "            'metapath_weight': float_path_weight,\n",
    "            'fold_epochs':200,\n",
    "            'save_dir':path_temp\n",
    "        }\n",
    "\n",
    "        '''create cell type dirs'''\n",
    "        sc_type_now = sc_type.replace(\" \", \"-\")\n",
    "        sc_type_now = sc_type_now.replace(\"/\", \"-\")\n",
    "        rc_type_now = rc_type.replace(\" \", \"-\")\n",
    "        rc_type_now = rc_type_now.replace(\"/\", \"-\")\n",
    "        sc_rc = sc_type_now + '_' + rc_type_now + '/'\n",
    "        sr_path = os.path.join(outs_symbol+'clrfgc_temp/'+current_time_str,  sc_rc)\n",
    "        os.makedirs(sr_path,exist_ok=True)\n",
    "        \n",
    "        '''create results dirs'''\n",
    "        embeds_path = os.path.join(sr_path,  'embeds/')\n",
    "        os.makedirs(embeds_path,exist_ok=True)\n",
    "        gene_exp_path = os.path.join(sr_path,  'gene_exp/')\n",
    "        os.makedirs(gene_exp_path,exist_ok=True)\n",
    "\n",
    "        '''save hdgi_para to json'''\n",
    "        hdgi_json_path = path + 'hdgi_para.json'\n",
    "        with open(hdgi_json_path, 'w') as json_file:\n",
    "            json.dump(hdgi_para_dict, json_file)\n",
    "            \n",
    "        '''check the type of tensor(outputs)'''\n",
    "        outputs = torch.tensor(cell_fea).float()\n",
    "        del cell_fea\n",
    "  \n",
    "            \n",
    "        '''HDGI---start----'''\n",
    "        runs = 10\n",
    "        att_list = []\n",
    "        auc_list = []\n",
    "        att_list.append(float_path_weight)\n",
    "        att_1_sum_list = []\n",
    "        pre_probs_list = []\n",
    "        cuda_num = \"cuda:4\"\n",
    "        device = torch.device(cuda_num if torch.cuda.is_available() else \"cpu\")\n",
    "        for i in range(runs):\n",
    "            epoch = 1\n",
    "            if hdgi_para_dict['noise_value'] > 0.0:\n",
    "                outputs = outputs.cpu().detach().numpy()\n",
    "                outputs = pd.DataFrame(outputs)\n",
    "                theta_L_values = np.linspace(0, 0.25, 10) \n",
    "                params = data_preprocess.fit_curve(theta_L_values, data_preprocess.calculate_theta_B_for_different_theta_L_values(outputs, hdgi_para_dict['noise_value'], theta_L_values))\n",
    "                outputs_with_noise = data_preprocess.add_noise_to_dataframe(outputs, hdgi_para_dict['noise_value'], theta_L_values[0])\n",
    "                outputs = torch.from_numpy(outputs_with_noise.values)\n",
    "            else:    \n",
    "                outputs = outputs\n",
    "            if hdgi_para_dict['preprocess_feat']:\n",
    "                features = RegChat_HDGI_train.preprocess_features_dense(outputs.cpu().detach().numpy())\n",
    "                features = torch.tensor(features).to(device)\n",
    "            else:\n",
    "                features = outputs\n",
    "            hdgi_para_dict['metapath_weight'] = torch.tensor(hdgi_para_dict['metapath_weight']).to(device)\n",
    "            att_1, _, xent_min_loss, embeds = RegChat_HDGI_train.train(features = features, labels = hdgi_labels, \n",
    "                                                patience = hdgi_para_dict['patience'], nb_class = hdgi_nb_class, \n",
    "                                                hid_units = hdgi_para_dict['hid_units'], shid = hdgi_para_dict['shid'],  \n",
    "                                                nonlinearity = hdgi_para_dict['nonlinearity'], \n",
    "                                                lr = hdgi_para_dict['lr'], l2_coef = hdgi_para_dict['l2_coef'], \n",
    "                                                adjs = adjs, sparse = hdgi_para_dict['sparse'], \n",
    "                                                nb_epochs = hdgi_para_dict['nb_epochs'], batch_size = hdgi_para_dict['batch_size'],\n",
    "                                                hgnn_epoch = epoch, metapath_weight = hdgi_para_dict['metapath_weight'], fixed_idx=fixed_idx,\n",
    "                                                original_idx_len=hdgi_para_dict['original_idx_len'],train_idx_len=hdgi_para_dict['train_idx_len'],\n",
    "                                                test_idx_len=hdgi_para_dict['test_idx_len'], \n",
    "                                                cuda_num=cuda_num,fold_epochs = hdgi_para_dict['fold_epochs'],\n",
    "                                                save_dir=hdgi_para_dict['save_dir'])\n",
    "            embeds_np = embeds.cpu().detach().numpy()\n",
    "            embeds_np_squeezed = np.squeeze(embeds_np, axis=0) \n",
    "            embeds_df = pd.DataFrame(embeds_np_squeezed)\n",
    "            sc_type_now = sc_type.replace(\" \", \"-\")     \n",
    "            rc_type_now = rc_type.replace(\" \", \"-\")\n",
    "            sc_type_now = sc_type_now.replace(\"/\", \"-\")\n",
    "            rc_type_now = rc_type_now.replace(\"/\", \"-\")\n",
    "            embeds_df.to_csv(embeds_path + sc_type_now+'_'+rc_type_now+'_embeds_runs_' + str(i) + '.csv')\n",
    "        \n",
    "            \n",
    "            print('----------HDGI_min_loss---------', xent_min_loss)\n",
    "            '''HDGI---end----'''\n",
    "\n",
    "            '''attention csv save'''\n",
    "            att_1_arr = np.array(att_1)\n",
    "            # att_2_arr = np.array(att_2)\n",
    "            att_1_arr = np.squeeze(att_1_arr, axis=2)\n",
    "            # att_2_arr = np.squeeze(att_2_arr, axis=2)\n",
    "            att_1 = pd.DataFrame(att_1_arr)     \n",
    "            # att_2 = pd.DataFrame(att_2_arr)\n",
    "            att_1.to_csv(sr_path + sc_type+'_'+rc_type+'_att_1_runs_' + str(i) + '.csv')\n",
    "            # att_2.to_csv(path + 'att_2.csv')\n",
    "            att_1_sum_list.append(att_1.sum(axis=0))\n",
    "\n",
    "            '''load pair label to attention figure'''\n",
    "            adjs_names_list = np.loadtxt('/home/nas2/biod/zhencaiwei/RegChatz/Datasets/Simulated_data_validST_withoutST/simulated_adj_files_all.txt', dtype='str')\n",
    "            adjs_names_list = list(adjs_names_list)\n",
    "            adjs_list_new = []\n",
    "            for item in adjs_names_list:\n",
    "                item_new = item[:-11]\n",
    "                adjs_list_new.append(item_new)\n",
    "            condition_signaling = hdgi_para_dict['LRpair']\n",
    "\n",
    "            '''Create a horizontal histogram'''\n",
    "            # data = list(att_1_arr[-1])\n",
    "            # data = att_1.sum(axis=0)\n",
    "            att_sum_data = att_1.sum(axis=0)\n",
    "            total_data = sum(att_sum_data) \n",
    "            data_weight = list(att_sum_data / total_data) \n",
    "            float_data_weight = [float('%.4f' % i) for i in data_weight]\n",
    "            data = float_data_weight\n",
    "            plt.barh(range(len(data)), data)\n",
    "            plt.yticks(range(len(data)), adjs_list_new) \n",
    "            plt.xlabel('attention score') \n",
    "            plt.ylabel('signaling pathway')\n",
    "            plt.tight_layout()\n",
    "            fig_name = sc_type+'_'+rc_type+'_attention_runs_' + str(i) + '.png'\n",
    "            fig_path = sr_path + fig_name \n",
    "            plt.savefig(fig_path)\n",
    "            plt.show()\n",
    "\n",
    "            '''compute auc'''\n",
    "            pre_probs = float_data_weight\n",
    "            if sc_type == 'CT1' and rc_type == 'CT2':\n",
    "                true_labels = [0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "                auc = roc_auc_score(true_labels, pre_probs)  \n",
    "            if sc_type == 'CT2' and rc_type == 'CT1':\n",
    "                true_labels = [0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,1]\n",
    "                auc = roc_auc_score(true_labels, pre_probs)\n",
    "            if sc_type == 'CT1' and rc_type == 'CT1':\n",
    "                true_labels = [0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "                auc = roc_auc_score(true_labels, pre_probs)\n",
    "            if sc_type == 'CT2' and rc_type == 'CT2':\n",
    "                true_labels = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0]\n",
    "                auc = roc_auc_score(true_labels, pre_probs) \n",
    "            if sc_type == 'CT1' and rc_type == 'CT3':\n",
    "                true_labels = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "                pre_probs_np = np.array(pre_probs)\n",
    "                true_labels_np = np.array(true_labels)\n",
    "                mse = np.mean((pre_probs_np - true_labels_np) ** 2)\n",
    "                auc = 1-mse\n",
    "            if sc_type == 'CT3' and rc_type == 'CT1':\n",
    "                true_labels = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "                pre_probs_np = np.array(pre_probs)\n",
    "                true_labels_np = np.array(true_labels)\n",
    "                mse = np.mean((pre_probs_np - true_labels_np) ** 2)                 \n",
    "                auc = 1-mse\n",
    "            if sc_type == 'CT2' and rc_type == 'CT3':\n",
    "                true_labels = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0]\n",
    "                auc = roc_auc_score(true_labels, pre_probs)\n",
    "            if sc_type == 'CT3' and rc_type == 'CT2':\n",
    "                true_labels = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0]\n",
    "                auc = roc_auc_score(true_labels, pre_probs)\n",
    "            if sc_type == 'CT3' and rc_type == 'CT3':\n",
    "                true_labels = [0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "                auc = roc_auc_score(true_labels, pre_probs)      \n",
    "            pre_probs = list(att_1_arr[-1])               \n",
    "            # auc = roc_auc_score(true_labels, pre_probs) \n",
    "            print('AUC: %.4f' % auc)\n",
    "            auc_list.append(auc)\n",
    "            att_list.append(att_1_arr[-1])\n",
    "            pre_probs_list.append(pre_probs)\n",
    "\n",
    "        print(sc_type+'_'+rc_type+'_auc_list', auc_list)\n",
    "        print(sc_type+'_'+rc_type+'_att_list', pd.DataFrame(att_list))\n",
    "        with open(sr_path + sc_type+'_'+rc_type+'_auc_list.csv', 'w') as file:\n",
    "            for i in range(len(auc_list)):\n",
    "                file.write(str(auc_list[i]) + '\\n')\n",
    "        with open(sr_path + sc_type+'_'+rc_type+'_att_list.csv', 'w') as file:\n",
    "            for i in range(len(att_list)):\n",
    "                file.write(str(list(att_list[i])) + '\\n')\n",
    "        att_list_df = pd.DataFrame(att_list)\n",
    "        att_list_df.to_csv(sr_path + sc_type+'_'+rc_type+'_att_list.csv')\n",
    "        att_1_sum_list_df = pd.DataFrame(att_1_sum_list)\n",
    "        att_1_sum_list_df.to_csv(sr_path + sc_type+'_'+rc_type+'_att_1_sum_list.csv')\n",
    "        pre_probs_list_df = pd.DataFrame(pre_probs_list)\n",
    "        pre_probs_list_df.to_csv(sr_path + sc_type+'_'+rc_type+'_pre_probs_list.csv')\n",
    "        \n",
    "        \n",
    "\n",
    "        ''' plot multiple runs attention figure'''\n",
    "        plt.figure()\n",
    "        x = adjs_list_new\n",
    "        y_list = pre_probs_list\n",
    "        for i in range(0,10):\n",
    "        # for i in range(0,3):\n",
    "            plt.plot(x, y_list[i], label=f'run {i}')   \n",
    "        plt.xticks(rotation=80)  \n",
    "        plt.title('all epochs')\n",
    "        plt.ylabel('attention score') \n",
    "        plt.xlabel('signaling pathway')\n",
    "        plt.tight_layout()\n",
    "        fig_name = sc_type+'_'+rc_type+'_attention_runs_all' + '.png'\n",
    "        fig_path = sr_path + fig_name \n",
    "        plt.savefig(fig_path)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        '''plot multiple runs attention zscore figure'''\n",
    "        z_scores = stats.zscore(pre_probs_list_df, axis=1, nan_policy='omit')\n",
    "        data_list = z_scores.values.tolist()    \n",
    "        plt.figure()\n",
    "        x = adjs_list_new\n",
    "        y_list = data_list\n",
    "        for i in range(0,10):\n",
    "        # for i in range(0, 3):\n",
    "            plt.plot(x, y_list[i], label=f'run {i}')   \n",
    "        plt.xticks(rotation=80) \n",
    "        plt.title('all epochs')\n",
    "        plt.ylabel('attention zscore') \n",
    "        plt.xlabel('signaling pathway')\n",
    "        plt.tight_layout()\n",
    "        fig_name = sc_type+'_'+rc_type+'_attention_zscore_runs_all' + '.png'\n",
    "        fig_path = sr_path + fig_name \n",
    "        plt.savefig(fig_path)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        '''plot multiple runs auc figure'''\n",
    "        plt.figure()\n",
    "        auc_avg = np.mean(auc_list)\n",
    "        print('auc_avg', auc_avg)\n",
    "        x = [1,2,3,4,5,6,7,8,9,10]\n",
    "        # x = [1,2,3]\n",
    "        y_list = auc_list \n",
    "        plt.plot(x, y_list)\n",
    "        for i,j in zip(x,y_list):\n",
    "            plt.text(i,j,'%.4f'%j,ha='center',va='bottom',fontsize=10)\n",
    "        plt.axhline(y=auc_avg, color='r', linestyle='--') \n",
    "        plt.text(5, auc_avg, 'avg: %.4f' % auc_avg, va='bottom', ha='right')\n",
    "        plt.title('AUC after 10 runs')\n",
    "        plt.ylabel('AUC')\n",
    "        plt.xlabel('runs')\n",
    "        plt.tight_layout()\n",
    "        fig_name = sc_type+'_'+rc_type+'_auc_runs_all' + '.png'\n",
    "        fig_path = sr_path + fig_name\n",
    "        plt.savefig(fig_path)\n",
    "        plt.show()\n",
    "\n",
    "        '''plot multiple runs auc figure'''\n",
    "        data_list = pre_probs_list\n",
    "        fig, axs = plt.subplots(2, 5, figsize=(15, 8))\n",
    "        for i, data in enumerate(data_list):\n",
    "            row = i // 5\n",
    "            col = i % 5 \n",
    "            axs[row, col].barh(range(len(data)), data)\n",
    "            axs[row, col].set_title(f'Run {i+1}')\n",
    "            axs[row, col].set_yticks(range(len(data)), adjs_list_new)\n",
    "            axs[row, col].set_xlabel('attention score')\n",
    "        fig.suptitle('all epochs')\n",
    "        fig_name = sc_type+'_'+rc_type+'_attention_runs_barh' + '.png'\n",
    "        fig_path = sr_path + fig_name \n",
    "        plt.savefig(fig_path)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        mean_att = np.mean(pre_probs_list, axis=0) \n",
    "        mean_att = mean_att.tolist()\n",
    "        for metapath, att_score in zip(adjs_names_list, mean_att):\n",
    "                metapath_attention_lists[metapath].append(att_score)\n",
    "        \n",
    "        z_scores_list = z_scores.values.tolist()\n",
    "        mean_att_zscore = np.mean(z_scores_list, axis=0)\n",
    "        mean_att_zscore = mean_att_zscore.tolist()\n",
    "        for metapath, att_score in zip(adjs_names_list, mean_att_zscore):\n",
    "                metapath_attention_zscore_lists[metapath].append(att_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metapath: L0_R3_F12_G9_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000186\n",
      "Metapath: L0_R8_F12_G15_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000239\n",
      "Metapath: L1_R1_F1_G1_adj.pickle\n",
      "          CT3\n",
      "CT3  0.432432\n",
      "Metapath: L1_R6_F17_G23_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000181\n",
      "Metapath: L2_R0_F4_G4_adj.pickle\n",
      "         CT3\n",
      "CT3  0.00029\n",
      "Metapath: L3_R0_F4_G4_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000233\n",
      "Metapath: L4_R3_F5_G6_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000254\n",
      "Metapath: L4_R5_F5_G7_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000151\n",
      "Metapath: L5_R7_F11_G17_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000203\n",
      "Metapath: L5_R7_F11_G32_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000514\n",
      "Metapath: L6_R9_F9_G2_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000743\n",
      "Metapath: L7_R4_F3_G4_adj.pickle\n",
      "          CT3\n",
      "CT3  0.001557\n",
      "Metapath: L7_R3_F10_G5_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000247\n",
      "Metapath: L7_R7_F11_G17_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000169\n",
      "Metapath: L7_R7_F11_G32_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000493\n",
      "Metapath: L7_R4_F10_G35_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000269\n",
      "Metapath: L8_R2_F11_G17_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000154\n",
      "Metapath: L8_R2_F11_G32_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000231\n",
      "Metapath: L9_R0_F4_G4_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000524\n",
      "Metapath: L10_R2_F11_G17_adj.pickle\n",
      "          CT3\n",
      "CT3  0.104734\n",
      "Metapath: L10_R2_F11_G32_adj.pickle\n",
      "          CT3\n",
      "CT3  0.008268\n",
      "Metapath: L10_R5_F3_G4_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000369\n",
      "Metapath: L11_R1_F11_G17_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000335\n",
      "Metapath: L11_R1_F11_G32_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000431\n",
      "Metapath: L12_R1_F15_G1_adj.pickle\n",
      "          CT3\n",
      "CT3  0.032274\n",
      "Metapath: L13_R0_F4_G4_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000279\n",
      "Metapath: L13_R2_F11_G17_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000302\n",
      "Metapath: L13_R2_F11_G32_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000166\n",
      "Metapath: L13_R6_F4_G4_adj.pickle\n",
      "          CT3\n",
      "CT3  0.000202\n",
      "Metapath: L14_R7_F8_G10_adj.pickle\n",
      "          CT3\n",
      "CT3  0.383904\n",
      "Metapath: L15_R0_F4_G4_adj.pickle\n",
      "          CT3\n",
      "CT3  0.029666\n",
      "Metapath: L0_R3_F12_G9_adj.pickle\n",
      "         CT3\n",
      "CT3 -0.27367\n",
      "Metapath: L0_R8_F12_G15_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.273268\n",
      "Metapath: L1_R1_F1_G1_adj.pickle\n",
      "          CT3\n",
      "CT3  3.294383\n",
      "Metapath: L1_R6_F17_G23_adj.pickle\n",
      "         CT3\n",
      "CT3 -0.27371\n",
      "Metapath: L2_R0_F4_G4_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.272887\n",
      "Metapath: L3_R0_F4_G4_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.273317\n",
      "Metapath: L4_R3_F5_G6_adj.pickle\n",
      "         CT3\n",
      "CT3 -0.27316\n",
      "Metapath: L4_R5_F5_G7_adj.pickle\n",
      "         CT3\n",
      "CT3 -0.27391\n",
      "Metapath: L5_R7_F11_G17_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.273546\n",
      "Metapath: L5_R7_F11_G32_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.270977\n",
      "Metapath: L6_R9_F9_G2_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.269465\n",
      "Metapath: L7_R4_F3_G4_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.263438\n",
      "Metapath: L7_R3_F10_G5_adj.pickle\n",
      "         CT3\n",
      "CT3 -0.27308\n",
      "Metapath: L7_R7_F11_G17_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.273784\n",
      "Metapath: L7_R7_F11_G32_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.271061\n",
      "Metapath: L7_R4_F10_G35_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.273053\n",
      "Metapath: L8_R2_F11_G17_adj.pickle\n",
      "         CT3\n",
      "CT3 -0.27389\n",
      "Metapath: L8_R2_F11_G32_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.273225\n",
      "Metapath: L9_R0_F4_G4_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.271155\n",
      "Metapath: L10_R2_F11_G17_adj.pickle\n",
      "          CT3\n",
      "CT3  0.616285\n",
      "Metapath: L10_R2_F11_G32_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.200996\n",
      "Metapath: L10_R5_F3_G4_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.272283\n",
      "Metapath: L11_R1_F11_G17_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.272559\n",
      "Metapath: L11_R1_F11_G32_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.271837\n",
      "Metapath: L12_R1_F15_G1_adj.pickle\n",
      "          CT3\n",
      "CT3  0.048294\n",
      "Metapath: L13_R0_F4_G4_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.272987\n",
      "Metapath: L13_R2_F11_G17_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.272792\n",
      "Metapath: L13_R2_F11_G32_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.273717\n",
      "Metapath: L13_R6_F4_G4_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.273556\n",
      "Metapath: L14_R7_F8_G10_adj.pickle\n",
      "          CT3\n",
      "CT3  3.079618\n",
      "Metapath: L15_R0_F4_G4_adj.pickle\n",
      "          CT3\n",
      "CT3 -0.027258\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''save metapath attention score, format one: multiple dfs'''                \n",
    "metapath_attention_dfs = {}\n",
    "for metapath, attention_list in metapath_attention_lists.items():\n",
    "    attention_array = np.array(attention_list)\n",
    "    attention_matrix = attention_array.reshape(len(scell_types), len(rcell_types))\n",
    "    attention_df = pd.DataFrame(attention_matrix, index=scell_types, columns=rcell_types)\n",
    "    metapath_attention_dfs[metapath] = attention_df\n",
    "for metapath, attention_df in metapath_attention_dfs.items():\n",
    "    print(f\"Metapath: {metapath}\")\n",
    "    print(attention_df)\n",
    "'''create mulitple dfs dirs'''\n",
    "dfs_path = os.path.join(outs_symbol+'clrfgc_temp/'+ current_time_str,  'clrfgc_attscore_dfs')\n",
    "os.makedirs(dfs_path)\n",
    "for metapath, attention_df in metapath_attention_dfs.items():     \n",
    "    attention_df.to_csv(f\"{dfs_path}/{metapath}_attention.csv\")\n",
    "\n",
    "\n",
    "    \n",
    "'''save metapath attention score, format two: one df'''\n",
    "combined_df = pd.DataFrame(index=pd.MultiIndex.from_product([scell_types, rcell_types]), columns=metapath_attention_dfs.keys())\n",
    "for metapath, attention_df in metapath_attention_dfs.items():\n",
    "    for row in attention_df.index:\n",
    "        for col in attention_df.columns:\n",
    "            combined_df.loc[(row, col), metapath] = attention_df.loc[row, col]\n",
    "'''create one df csv file'''\n",
    "combined_df_path = path_temp + 'attscore_combined_df.csv'\n",
    "combined_df.to_csv(combined_df_path, sep=',')\n",
    "\n",
    "\n",
    "'''save metapath attention zscore, format one: multiple dfs'''\n",
    "metapath_attention_zscore_dfs = {}\n",
    "for metapath, attention_list in metapath_attention_zscore_lists.items():\n",
    "    attention_array = np.array(attention_list)\n",
    "    attention_matrix = attention_array.reshape(len(scell_types), len(rcell_types))\n",
    "    attention_df = pd.DataFrame(attention_matrix, index=scell_types, columns=rcell_types)\n",
    "    metapath_attention_zscore_dfs[metapath] = attention_df\n",
    "for metapath, attention_df in metapath_attention_zscore_dfs.items():\n",
    "    print(f\"Metapath: {metapath}\")\n",
    "    print(attention_df)\n",
    "'''create mulitple dfs dirs'''\n",
    "dfs_path = os.path.join(outs_symbol+'clrfgc_temp/'+ current_time_str,  'clrfgc_attscore_zscore_dfs')\n",
    "os.makedirs(dfs_path)\n",
    "for metapath, attention_df in metapath_attention_zscore_dfs.items():     \n",
    "    attention_df.to_csv(f\"{dfs_path}/{metapath}_attention_zscore.csv\")\n",
    "    \n",
    "\n",
    "'''save metapath attention zscore, format two: one df'''\n",
    "combined_zscore_df = pd.DataFrame(index=pd.MultiIndex.from_product([scell_types, rcell_types]), columns=metapath_attention_zscore_dfs.keys())\n",
    "for metapath, attention_df in metapath_attention_zscore_dfs.items():\n",
    "    for row in attention_df.index:\n",
    "        for col in attention_df.columns:\n",
    "            combined_zscore_df.loc[(row, col), metapath] = attention_df.loc[row, col]\n",
    "'''create one df csv file'''\n",
    "combined_zscore_df_path = path_temp + 'attscore_zscore_combined_df.csv'\n",
    "combined_zscore_df.to_csv(combined_zscore_df_path, sep=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
